{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gutenberg', 'gutenberg.zip', 'reuters.zip', 'stopwords', 'stopwords.zip', 'wordnet', 'wordnet.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find(\"corpora\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/brown\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Gav/nltk_data'\n    - 'C:\\\\Users\\\\Gav\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gav\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gav\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/brown.zip/brown/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Gav/nltk_data'\n    - 'C:\\\\Users\\\\Gav\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gav\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gav\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9c2949b3e981>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbrown\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbrown\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/brown\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Gav/nltk_data'\n    - 'C:\\\\Users\\\\Gav\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gav\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gav\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet=nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
    "hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "  >>> nltk.download('gutenberg')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet=nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
    "hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = pd.read_txt(r\"E:\\Documents\\Career\\XQ Institute\\IG Post Content from Social Growth Tracker.txt\")\n",
    "grocery.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = pd.read_csv(r\"E:\\Documents\\Career\\XQ Institute\\IG Post Content from Social Growth Tracker.txt\")\n",
    "grocery.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IG = \"\"\"\"These are my two best high school friends, Nicole and Jovinka. Essentially, they’ve come to my rescue when I faced the obstacle of making friends and acquaintances at the new school after having immigrated to the US. Some of us know how lonesome it can be at a place full of people who are complete strangers to you. Especially when it’s a new area you move to. Even moving to the town next to yours can cause a pretty major cultural difference; a place 10 miles away might seem peculiar and unknown. After traveling almost 6,000 miles across the ocean and vast lands, the cultural difference seemed so immense to me that the struggle to simply have a conversation with someone was inevitable.\n",
    "\n",
    "#ByStudentsForStudents | 📸 @bl_cc\"\n",
    "\"In high school, exploring the world (in any capacity) is resistance—resistance toward oppressive systems like ageism that tell students our ideas don’t matter...resistance toward police that bar our generation from reaching our full potential.\n",
    "\n",
    "The art on the walls in Bethlehem are forms of resistance that speak to high schoolers on the other side of the world and resonates with students here in Los Angeles and across the United States.\n",
    "\n",
    "#ByStudentsForStudents | 📸 @huevitosfritoss\"\n",
    "\"This is my best friend, Emily, and she completely impacted my high school experience. She's been my best friend for the past four years, and I don't know where I would be without her... I imagine it would probably be somewhere with less glitter and less love. I've grown so much with Emily by my side. We've experienced so much from laughter, screams, cries and more together. The moments and memories I share with her are absolutely amazing, and I cherish them with everything I have.\n",
    "\n",
    "#ByStudentsForStudents | 📸 @ghhostpepper\"\n",
    "\"NO RECESS | As a kid in elementary school, all I looked forward to was recess.\n",
    "\n",
    "I can remember staring at the clock until it was time to go out to the playground to run around and play tag with my friends...or pretend I was a Ninja Warrior contestant...or do cartwheels because all I wanted to do was play as hard as I was studying to learn everything a 2nd grader ought to know.\n",
    "\n",
    "Now, I’m older, and I don’t stare at the clock until it strikes the time for recess anymore. Instead, now more than ever, I make sure that my work is my recess and my recess is my work.\n",
    "\n",
    "#ByStudentsForStudents | 📸 @el.rey.sierra\"\n",
    "\"This is my friend, Sascha. Together we are the biggest goofballs you'd ever meet. We have so many things in common, it's like we're the same person. She's the type of person that will give you her honest opinion and will always ask you to join her on an adventure. Who's your Sascha?\n",
    "#ByStudentsForStudents | 📸 @aandrea.flores\"\n",
    "\"People always ask me what I’m going to do with my life, and sometimes I have to keep myself from laughing.\n",
    "\n",
    "I’m in high school. I'm 16 years old.\n",
    "\n",
    "There’s no way I know what my life’s purpose is. There’s no way I know for certain what single path I’m going down.\n",
    "\n",
    "What I do know is that I’m still climbing this mountain of life, and I’m doing things that I love every day, and with people I love. While there have been some major roadblocks on this journey, I’ve always found a way through, despite any bruises or scratches I get along the way...I ALWAYS make it through. You can, too.\n",
    "\n",
    "#ByStudentsForStudents | 📸 @takenbycali\"\n",
    "\"There’s so much busywork in high school. You can spend hours working on endless sheets of paper that all teach you the same thing.\n",
    " \n",
    " You spend seven hours at school learning, just to end up going home and spending hours doing the homework assignment for the same subjects with the same information. And sometimes, you just need a break from all the busywork.\n",
    " \n",
    " #ByStudentsForStudents | 📸 @ahguil\"\n",
    "\"In high school, there's a lot to juggle...homework, projects, chores, extracurricular activities, sometimes even a part-time job...the list goes on.\n",
    " \n",
    " Sometimes, it's nice to just take a break and let your imagination run wild. So, I did.\n",
    " \n",
    " I turned off my lights, closed the blinds, and got lost in this TV background. So, I decided to capture the moment.\n",
    " \n",
    " As an artist and photographer, I’m always thinking about how to harness my creativity. How do you harness yours?\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IG_tokens = word_tokenize(IG)\n",
    "IG_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IG_tokens = word_tokenize(IG)\n",
    "IG_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(IG_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in IG_tokens:\n",
    "    fdist[word.lower()]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in IG_tokens:\n",
    "    fdist[word.lower()]+=1\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist['friends']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fdist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_top10 = fdist.most_common(10)\n",
    "fdist_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "IG_blank=blankline_tokenize(IG)\n",
    "len(IG_blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IG_blank[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Hey what's going on, what's going on\"\n",
    "quotes_tokens = nltk.word_tokenize(string)\n",
    "quotes_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Hey what is going on, what is going on\"\n",
    "quotes_tokens = nltk.word_tokenize(string)\n",
    "quotes_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_bigrams = list(nltk.bigrams(quotes_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_brigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qutoes_bigrams/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_trigrams = list(nltk.trigrams(quotes_tokens))\n",
    "quotes_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_ngrams= list(nltk.ngrams(quotes_tokens, 5))\n",
    "quotes_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.stem(\"having\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give\n",
      "giving\n",
      "given\n",
      "gave\n"
     ]
    }
   ],
   "source": [
    "words_to_stem=[\"give\",\"giving\",\"given\",\"gave\"]\n",
    "for words in words_to_stem:\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:give\n",
      "given:given\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "words_to_stem=[\"give\",\"giving\",\"given\",\"gave\"]\n",
    "for words in words_to_stem:\n",
    "    print(words+ \":\" +pst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:give\n",
      "given:given\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst=LancasterStemmer()\n",
    "for words in words_to_stem:\n",
    "    print(words+ \":\" +pst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "sbst=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:give\n",
      "given:given\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "for words in words_to_stem:\n",
    "    print(words+ \":\" +pst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lem=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for words in words_to_stem:\n",
    "    print:(words+ \":\" +word_lem.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Gav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  >>> import nltk\n",
    "  >>> nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:giving\n",
      "given:given\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "for words in words_to_stem:\n",
    "    print(words+ \":\" +word_lem.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  >>> import nltk\n",
    "  >>> nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 78),\n",
       " (',', 72),\n",
       " ('i', 70),\n",
       " ('the', 62),\n",
       " ('and', 48),\n",
       " ('to', 48),\n",
       " ('my', 32),\n",
       " ('’', 32),\n",
       " (\"''\", 28),\n",
       " ('a', 26)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "punctuation=re.compile(r'[-.?!,:;()|0-9]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_punctuation=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_punctuation=[]\n",
    "for words in IG_tokens:\n",
    "    word=punctuation.sub(\"\",words)\n",
    "    if len(word)>0:\n",
    "        post_punctuation.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['``',\n",
       " 'These',\n",
       " 'are',\n",
       " 'my',\n",
       " 'two',\n",
       " 'best',\n",
       " 'high',\n",
       " 'school',\n",
       " 'friends',\n",
       " 'Nicole',\n",
       " 'and',\n",
       " 'Jovinka',\n",
       " 'Essentially',\n",
       " 'they',\n",
       " '’',\n",
       " 've',\n",
       " 'come',\n",
       " 'to',\n",
       " 'my',\n",
       " 'rescue',\n",
       " 'when',\n",
       " 'I',\n",
       " 'faced',\n",
       " 'the',\n",
       " 'obstacle',\n",
       " 'of',\n",
       " 'making',\n",
       " 'friends',\n",
       " 'and',\n",
       " 'acquaintances',\n",
       " 'at',\n",
       " 'the',\n",
       " 'new',\n",
       " 'school',\n",
       " 'after',\n",
       " 'having',\n",
       " 'immigrated',\n",
       " 'to',\n",
       " 'the',\n",
       " 'US',\n",
       " 'Some',\n",
       " 'of',\n",
       " 'us',\n",
       " 'know',\n",
       " 'how',\n",
       " 'lonesome',\n",
       " 'it',\n",
       " 'can',\n",
       " 'be',\n",
       " 'at',\n",
       " 'a',\n",
       " 'place',\n",
       " 'full',\n",
       " 'of',\n",
       " 'people',\n",
       " 'who',\n",
       " 'are',\n",
       " 'complete',\n",
       " 'strangers',\n",
       " 'to',\n",
       " 'you',\n",
       " 'Especially',\n",
       " 'when',\n",
       " 'it',\n",
       " '’',\n",
       " 's',\n",
       " 'a',\n",
       " 'new',\n",
       " 'area',\n",
       " 'you',\n",
       " 'move',\n",
       " 'to',\n",
       " 'Even',\n",
       " 'moving',\n",
       " 'to',\n",
       " 'the',\n",
       " 'town',\n",
       " 'next',\n",
       " 'to',\n",
       " 'yours',\n",
       " 'can',\n",
       " 'cause',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'major',\n",
       " 'cultural',\n",
       " 'difference',\n",
       " 'a',\n",
       " 'place',\n",
       " 'miles',\n",
       " 'away',\n",
       " 'might',\n",
       " 'seem',\n",
       " 'peculiar',\n",
       " 'and',\n",
       " 'unknown',\n",
       " 'After',\n",
       " 'traveling',\n",
       " 'almost',\n",
       " 'miles',\n",
       " 'across',\n",
       " 'the',\n",
       " 'ocean',\n",
       " 'and',\n",
       " 'vast',\n",
       " 'lands',\n",
       " 'the',\n",
       " 'cultural',\n",
       " 'difference',\n",
       " 'seemed',\n",
       " 'so',\n",
       " 'immense',\n",
       " 'to',\n",
       " 'me',\n",
       " 'that',\n",
       " 'the',\n",
       " 'struggle',\n",
       " 'to',\n",
       " 'simply',\n",
       " 'have',\n",
       " 'a',\n",
       " 'conversation',\n",
       " 'with',\n",
       " 'someone',\n",
       " 'was',\n",
       " 'inevitable',\n",
       " '#',\n",
       " 'ByStudentsForStudents',\n",
       " '📸',\n",
       " '@',\n",
       " 'bl_cc',\n",
       " \"''\",\n",
       " \"''\",\n",
       " 'In',\n",
       " 'high',\n",
       " 'school',\n",
       " 'exploring',\n",
       " 'the',\n",
       " 'world',\n",
       " 'in',\n",
       " 'any',\n",
       " 'capacity',\n",
       " 'is',\n",
       " 'resistance—resistance',\n",
       " 'toward',\n",
       " 'oppressive',\n",
       " 'systems',\n",
       " 'like',\n",
       " 'ageism',\n",
       " 'that',\n",
       " 'tell',\n",
       " 'students',\n",
       " 'our',\n",
       " 'ideas',\n",
       " 'don',\n",
       " '’',\n",
       " 't',\n",
       " 'matter',\n",
       " 'resistance',\n",
       " 'toward',\n",
       " 'police',\n",
       " 'that',\n",
       " 'bar',\n",
       " 'our',\n",
       " 'generation',\n",
       " 'from',\n",
       " 'reaching',\n",
       " 'our',\n",
       " 'full',\n",
       " 'potential',\n",
       " 'The',\n",
       " 'art',\n",
       " 'on',\n",
       " 'the',\n",
       " 'walls',\n",
       " 'in',\n",
       " 'Bethlehem',\n",
       " 'are',\n",
       " 'forms',\n",
       " 'of',\n",
       " 'resistance',\n",
       " 'that',\n",
       " 'speak',\n",
       " 'to',\n",
       " 'high',\n",
       " 'schoolers',\n",
       " 'on',\n",
       " 'the',\n",
       " 'other',\n",
       " 'side',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'and',\n",
       " 'resonates',\n",
       " 'with',\n",
       " 'students',\n",
       " 'here',\n",
       " 'in',\n",
       " 'Los',\n",
       " 'Angeles',\n",
       " 'and',\n",
       " 'across',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States',\n",
       " '#',\n",
       " 'ByStudentsForStudents',\n",
       " '📸',\n",
       " '@',\n",
       " 'huevitosfritoss',\n",
       " \"''\",\n",
       " \"''\",\n",
       " 'This',\n",
       " 'is',\n",
       " 'my',\n",
       " 'best',\n",
       " 'friend',\n",
       " 'Emily',\n",
       " 'and',\n",
       " 'she',\n",
       " 'completely',\n",
       " 'impacted',\n",
       " 'my',\n",
       " 'high',\n",
       " 'school',\n",
       " 'experience',\n",
       " 'She',\n",
       " \"'s\",\n",
       " 'been',\n",
       " 'my',\n",
       " 'best',\n",
       " 'friend',\n",
       " 'for',\n",
       " 'the',\n",
       " 'past',\n",
       " 'four',\n",
       " 'years',\n",
       " 'and',\n",
       " 'I',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'where',\n",
       " 'I',\n",
       " 'would',\n",
       " 'be',\n",
       " 'without',\n",
       " 'her',\n",
       " 'I',\n",
       " 'imagine',\n",
       " 'it',\n",
       " 'would',\n",
       " 'probably',\n",
       " 'be',\n",
       " 'somewhere',\n",
       " 'with',\n",
       " 'less',\n",
       " 'glitter',\n",
       " 'and',\n",
       " 'less',\n",
       " 'love',\n",
       " 'I',\n",
       " \"'ve\",\n",
       " 'grown',\n",
       " 'so',\n",
       " 'much',\n",
       " 'with',\n",
       " 'Emily',\n",
       " 'by',\n",
       " 'my',\n",
       " 'side',\n",
       " 'We',\n",
       " \"'ve\",\n",
       " 'experienced',\n",
       " 'so',\n",
       " 'much',\n",
       " 'from',\n",
       " 'laughter',\n",
       " 'screams',\n",
       " 'cries',\n",
       " 'and',\n",
       " 'more',\n",
       " 'together',\n",
       " 'The',\n",
       " 'moments',\n",
       " 'and',\n",
       " 'memories',\n",
       " 'I',\n",
       " 'share',\n",
       " 'with',\n",
       " 'her',\n",
       " 'are',\n",
       " 'absolutely',\n",
       " 'amazing',\n",
       " 'and',\n",
       " 'I',\n",
       " 'cherish',\n",
       " 'them',\n",
       " 'with',\n",
       " 'everything',\n",
       " 'I',\n",
       " 'have',\n",
       " '#',\n",
       " 'ByStudentsForStudents',\n",
       " '📸',\n",
       " '@',\n",
       " 'ghhostpepper',\n",
       " \"''\",\n",
       " \"''\",\n",
       " 'NO',\n",
       " 'RECESS',\n",
       " 'As',\n",
       " 'a',\n",
       " 'kid',\n",
       " 'in',\n",
       " 'elementary',\n",
       " 'school',\n",
       " 'all',\n",
       " 'I',\n",
       " 'looked',\n",
       " 'forward',\n",
       " 'to',\n",
       " 'was',\n",
       " 'recess',\n",
       " 'I',\n",
       " 'can',\n",
       " 'remember',\n",
       " 'staring',\n",
       " 'at',\n",
       " 'the',\n",
       " 'clock',\n",
       " 'until',\n",
       " 'it',\n",
       " 'was',\n",
       " 'time',\n",
       " 'to',\n",
       " 'go',\n",
       " 'out',\n",
       " 'to',\n",
       " 'the',\n",
       " 'playground',\n",
       " 'to',\n",
       " 'run',\n",
       " 'around',\n",
       " 'and',\n",
       " 'play',\n",
       " 'tag',\n",
       " 'with',\n",
       " 'my',\n",
       " 'friends',\n",
       " 'or',\n",
       " 'pretend',\n",
       " 'I',\n",
       " 'was',\n",
       " 'a',\n",
       " 'Ninja',\n",
       " 'Warrior',\n",
       " 'contestant',\n",
       " 'or',\n",
       " 'do',\n",
       " 'cartwheels',\n",
       " 'because',\n",
       " 'all',\n",
       " 'I',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'do',\n",
       " 'was',\n",
       " 'play',\n",
       " 'as',\n",
       " 'hard',\n",
       " 'as',\n",
       " 'I',\n",
       " 'was',\n",
       " 'studying',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'everything',\n",
       " 'a',\n",
       " 'nd',\n",
       " 'grader',\n",
       " 'ought',\n",
       " 'to',\n",
       " 'know',\n",
       " 'Now',\n",
       " 'I',\n",
       " '’',\n",
       " 'm',\n",
       " 'older',\n",
       " 'and',\n",
       " 'I',\n",
       " 'don',\n",
       " '’',\n",
       " 't',\n",
       " 'stare',\n",
       " 'at',\n",
       " 'the',\n",
       " 'clock',\n",
       " 'until',\n",
       " 'it',\n",
       " 'strikes',\n",
       " 'the',\n",
       " 'time',\n",
       " 'for',\n",
       " 'recess',\n",
       " 'anymore',\n",
       " 'Instead',\n",
       " 'now',\n",
       " 'more',\n",
       " 'than',\n",
       " 'ever',\n",
       " 'I',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'that',\n",
       " 'my',\n",
       " 'work',\n",
       " 'is',\n",
       " 'my',\n",
       " 'recess',\n",
       " 'and',\n",
       " 'my',\n",
       " 'recess',\n",
       " 'is',\n",
       " 'my',\n",
       " 'work',\n",
       " '#',\n",
       " 'ByStudentsForStudents',\n",
       " '📸',\n",
       " '@',\n",
       " 'elreysierra',\n",
       " \"''\",\n",
       " \"''\",\n",
       " 'This',\n",
       " 'is',\n",
       " 'my',\n",
       " 'friend',\n",
       " 'Sascha',\n",
       " 'Together',\n",
       " 'we',\n",
       " 'are',\n",
       " 'the',\n",
       " 'biggest',\n",
       " 'goofballs',\n",
       " 'you',\n",
       " \"'d\",\n",
       " 'ever',\n",
       " 'meet',\n",
       " 'We',\n",
       " 'have',\n",
       " 'so',\n",
       " 'many',\n",
       " 'things',\n",
       " 'in',\n",
       " 'common',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'like',\n",
       " 'we',\n",
       " \"'re\",\n",
       " 'the',\n",
       " 'same',\n",
       " 'person',\n",
       " 'She',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'type',\n",
       " 'of',\n",
       " 'person',\n",
       " 'that',\n",
       " 'will',\n",
       " 'give',\n",
       " 'you',\n",
       " 'her',\n",
       " 'honest',\n",
       " 'opinion',\n",
       " 'and',\n",
       " 'will',\n",
       " 'always',\n",
       " 'ask',\n",
       " 'you',\n",
       " 'to',\n",
       " 'join',\n",
       " 'her',\n",
       " 'on',\n",
       " 'an',\n",
       " 'adventure',\n",
       " 'Who',\n",
       " \"'s\",\n",
       " 'your',\n",
       " 'Sascha',\n",
       " '#',\n",
       " 'ByStudentsForStudents',\n",
       " '📸',\n",
       " '@',\n",
       " 'aandreaflores',\n",
       " \"''\",\n",
       " \"''\",\n",
       " 'People',\n",
       " 'always',\n",
       " 'ask',\n",
       " 'me',\n",
       " 'what',\n",
       " 'I',\n",
       " '’',\n",
       " 'm',\n",
       " 'going',\n",
       " 'to',\n",
       " 'do',\n",
       " 'with',\n",
       " 'my',\n",
       " 'life',\n",
       " 'and',\n",
       " 'sometimes',\n",
       " 'I',\n",
       " 'have',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'myself',\n",
       " 'from',\n",
       " 'laughing',\n",
       " 'I',\n",
       " '’',\n",
       " 'm',\n",
       " 'in',\n",
       " 'high',\n",
       " 'school',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'years',\n",
       " 'old',\n",
       " 'There',\n",
       " '’',\n",
       " 's',\n",
       " 'no',\n",
       " 'way',\n",
       " 'I',\n",
       " 'know',\n",
       " 'what',\n",
       " 'my',\n",
       " 'life',\n",
       " '’',\n",
       " 's',\n",
       " 'purpose',\n",
       " 'is',\n",
       " 'There',\n",
       " '’',\n",
       " 's',\n",
       " 'no',\n",
       " 'way',\n",
       " 'I',\n",
       " 'know',\n",
       " 'for',\n",
       " 'certain',\n",
       " 'what',\n",
       " 'single',\n",
       " 'path',\n",
       " 'I',\n",
       " '’',\n",
       " 'm',\n",
       " 'going',\n",
       " 'down',\n",
       " 'What',\n",
       " 'I',\n",
       " 'do',\n",
       " 'know',\n",
       " 'is',\n",
       " 'that',\n",
       " 'I',\n",
       " '’',\n",
       " 'm',\n",
       " 'still',\n",
       " 'climbing',\n",
       " 'this',\n",
       " 'mountain',\n",
       " 'of',\n",
       " 'life',\n",
       " 'and',\n",
       " 'I',\n",
       " '’',\n",
       " 'm',\n",
       " 'doing',\n",
       " 'things',\n",
       " 'that',\n",
       " 'I',\n",
       " 'love',\n",
       " 'every',\n",
       " 'day',\n",
       " 'and',\n",
       " 'with',\n",
       " 'people',\n",
       " 'I',\n",
       " 'love',\n",
       " 'While',\n",
       " 'there',\n",
       " 'have',\n",
       " 'been',\n",
       " 'some',\n",
       " 'major',\n",
       " 'roadblocks',\n",
       " 'on',\n",
       " 'this',\n",
       " 'journey',\n",
       " 'I',\n",
       " '’',\n",
       " 've',\n",
       " 'always',\n",
       " 'found',\n",
       " 'a',\n",
       " 'way',\n",
       " 'through',\n",
       " 'despite',\n",
       " 'any',\n",
       " 'bruises',\n",
       " 'or',\n",
       " 'scratches',\n",
       " 'I',\n",
       " 'get',\n",
       " 'along',\n",
       " 'the',\n",
       " 'way',\n",
       " 'I',\n",
       " 'ALWAYS',\n",
       " 'make',\n",
       " 'it',\n",
       " 'through',\n",
       " 'You',\n",
       " 'can',\n",
       " 'too',\n",
       " '#',\n",
       " 'ByStudentsForStudents',\n",
       " '📸',\n",
       " '@',\n",
       " 'takenbycali',\n",
       " \"''\",\n",
       " \"''\",\n",
       " 'There',\n",
       " '’',\n",
       " 's',\n",
       " 'so',\n",
       " 'much',\n",
       " 'busywork',\n",
       " 'in',\n",
       " 'high',\n",
       " 'school',\n",
       " 'You',\n",
       " 'can',\n",
       " 'spend',\n",
       " 'hours',\n",
       " 'working',\n",
       " 'on',\n",
       " 'endless',\n",
       " 'sheets',\n",
       " 'of',\n",
       " 'paper',\n",
       " 'that',\n",
       " 'all',\n",
       " 'teach',\n",
       " 'you',\n",
       " 'the',\n",
       " 'same',\n",
       " 'thing',\n",
       " 'You',\n",
       " 'spend',\n",
       " 'seven',\n",
       " 'hours',\n",
       " 'at',\n",
       " 'school',\n",
       " 'learning',\n",
       " 'just',\n",
       " 'to',\n",
       " 'end',\n",
       " 'up',\n",
       " 'going',\n",
       " 'home',\n",
       " 'and',\n",
       " 'spending',\n",
       " 'hours',\n",
       " 'doing',\n",
       " 'the',\n",
       " 'homework',\n",
       " 'assignment',\n",
       " 'for',\n",
       " 'the',\n",
       " 'same',\n",
       " 'subjects',\n",
       " 'with',\n",
       " 'the',\n",
       " 'same',\n",
       " 'information',\n",
       " 'And',\n",
       " 'sometimes',\n",
       " 'you',\n",
       " 'just',\n",
       " 'need',\n",
       " 'a',\n",
       " 'break',\n",
       " 'from',\n",
       " 'all',\n",
       " 'the',\n",
       " 'busywork',\n",
       " '#',\n",
       " 'ByStudentsForStudents',\n",
       " '📸',\n",
       " '@',\n",
       " 'ahguil',\n",
       " \"''\",\n",
       " \"''\",\n",
       " 'In',\n",
       " 'high',\n",
       " 'school',\n",
       " 'there',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'lot',\n",
       " 'to',\n",
       " 'juggle',\n",
       " 'homework',\n",
       " 'projects',\n",
       " 'chores',\n",
       " 'extracurricular',\n",
       " 'activities',\n",
       " 'sometimes',\n",
       " 'even',\n",
       " 'a',\n",
       " 'parttime',\n",
       " 'job',\n",
       " 'the',\n",
       " 'list',\n",
       " 'goes',\n",
       " 'on',\n",
       " 'Sometimes',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'nice',\n",
       " 'to',\n",
       " 'just',\n",
       " 'take',\n",
       " 'a',\n",
       " 'break',\n",
       " 'and',\n",
       " 'let',\n",
       " 'your',\n",
       " 'imagination',\n",
       " 'run',\n",
       " 'wild',\n",
       " 'So',\n",
       " 'I',\n",
       " 'did',\n",
       " 'I',\n",
       " 'turned',\n",
       " 'off',\n",
       " 'my',\n",
       " 'lights',\n",
       " 'closed',\n",
       " 'the',\n",
       " 'blinds',\n",
       " 'and',\n",
       " 'got',\n",
       " 'lost',\n",
       " 'in',\n",
       " 'this',\n",
       " 'TV',\n",
       " 'background',\n",
       " 'So',\n",
       " 'I',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'capture',\n",
       " 'the',\n",
       " 'moment',\n",
       " 'As',\n",
       " 'an',\n",
       " 'artist',\n",
       " 'and',\n",
       " 'photographer',\n",
       " 'I',\n",
       " '’',\n",
       " 'm',\n",
       " 'always',\n",
       " 'thinking',\n",
       " 'about',\n",
       " 'how',\n",
       " 'to',\n",
       " 'harness',\n",
       " 'my',\n",
       " 'creativity',\n",
       " 'How',\n",
       " 'do',\n",
       " 'you',\n",
       " 'harness',\n",
       " 'yours']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Timothy is a natural when it comes to drawing\"\n",
    "sent_tokens = word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Timothy', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('a', 'DT')]\n",
      "[('natural', 'JJ')]\n",
      "[('when', 'WRB')]\n",
      "[('it', 'PRP')]\n",
      "[('comes', 'VBZ')]\n",
      "[('to', 'TO')]\n",
      "[('drawing', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "for token in sent_tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Gav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_sent = \"The US President stay in the White House\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_tokens = word_tokenize(NE_sent)\n",
    "NE_tags = nltk.pos_tag(NE_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION US/NNP)\n",
      "  President/NNP\n",
      "  stay/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (FACILITY White/NNP House/NNP))\n"
     ]
    }
   ],
   "source": [
    "NE_NER = ne_chunk(NE_tags)\n",
    "print(NE_NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Gav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Gav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  >>> import nltk\n",
    "  >>> nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('cat', 'NN'),\n",
       " ('ate', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('mouse', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('was', 'VBD'),\n",
       " ('after', 'IN'),\n",
       " ('fresh', 'JJ'),\n",
       " ('cheese', 'NN')]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = \"The big cat ate the little mouse who was after fresh cheese\"\n",
    "new_tokens = nltk.pos_tag(word_tokenize(new))\n",
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_np = r\"NP: {<DT>?<JJ>*<NN>}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_parser = nltk.RegexpParser(grammar_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
